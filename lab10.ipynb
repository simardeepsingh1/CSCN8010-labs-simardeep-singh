{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Lab 10\n",
    "## Student Name: `Simardeep Singh`\n",
    "## Student Roll Number:`8976948`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUCTION\n",
    "### Lab 10: Vanilla CNN and Fine-Tuning VGG16 for Dogs vs Cats Classification\n",
    "\n",
    "Welcome to Lab 10! In this session, we'll delve into a common practice among Deep Learning Engineers: fine-tuning an existing model to suit a specific task. Specifically, we will focus on the task of classifying images as either dogs or cats. This involves taking a pre-existing model, which is somewhat related to our task, and tweaking it to perform better on our specific dataset.\n",
    "\n",
    "#### Objective:\n",
    "To understand and implement the process of fine-tuning a pre-trained model (VGG16) and compare its performance with a custom-built Vanilla CNN on the task of classifying images into two categories: dogs and cats.\n",
    "\n",
    "\n",
    "\n",
    "#### Obtain the Data:\n",
    "Download the Dogs vs Cats dataset as outlined in the CSCN8010 class notebook. Ensure you have the correct folder structure and files before proceeding.\n",
    "\n",
    "#### Exploratory Data Analysis (EDA):\n",
    "Perform an exploratory analysis of the dataset. Include relevant graphs, statistics, and insights to understand the composition and characteristics of the data. Points to consider:\n",
    "- Distribution of classes (dogs vs cats)\n",
    "- Sample images from each class\n",
    "- Image dimensions and channels\n",
    "\n",
    "#### Model Training:\n",
    "In this section, we will train two different neural networks and evaluate their performance.\n",
    "\n",
    "#### 1. Vanilla CNN:\n",
    "- **Objective**: Define and train a custom Convolutional Neural Network (CNN) from scratch.\n",
    "- **Details**: Construct the architecture, specifying the layers, activation functions, and optimizer. Train the model on the Dogs vs Cats dataset.\n",
    "- **Evaluation**: Utilize callbacks to save the best version of the model and use validation data to check for overfitting.\n",
    "\n",
    "#### 2. Fine-Tuning VGG16:\n",
    "- **Objective**: Fine-tune the pre-trained VGG16 model (originally trained on ImageNet) for our specific task.\n",
    "- **Details**: Modify the top layers of the VGG16 model to suit our binary classification task. Ensure the initial layers remain frozen, and only the new layers are trainable.\n",
    "- **Evaluation**: Train the model on the dataset, using validation splits to monitor for overfitting. Plot relevant performance graphs.\n",
    "\n",
    "#### Performance Evaluation:\n",
    "After training both models, evaluate their performance based on the following metrics:\n",
    "- Accuracy\n",
    "- Confusion Matrix\n",
    "- Precision, Recall, F1-score\n",
    "- Precision-Recall Curve\n",
    "\n",
    "#### Case Studies:\n",
    "Explore specific instances where the models failed to predict correctly. Analyze and discuss possible reasons for these misclassifications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIBRARIES\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models ,optimizers\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1 Obtain the Data: Get the Dogs vs Cats dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define original and new directory paths\n",
    "original_data_dir = pathlib.Path(\"C:\\\\Foundations_of_Machine_Learning_Frameworks_lab\\\\Labs\\\\lab1\\\\CSCN8010-labs-simardeep-singh\\\\data\\\\dogVsCatData\\\\train\")\n",
    "subset_data_dir = pathlib.Path(\"C:\\\\Foundations_of_Machine_Learning_Frameworks_lab\\\\Labs\\\\lab1\\\\CSCN8010-labs-simardeep-singh\\\\data\\\\dogVsCatData\\\\subset\")\n",
    "\n",
    "def create_data_subset(subset_name, start_index, end_index):\n",
    "    \"\"\"Create subsets of data for cats and dogs.\"\"\"\n",
    "    for category in (\"cat\", \"dog\"):\n",
    "        # Create new directory path for the subset\n",
    "        subset_dir = subset_data_dir / subset_name / category\n",
    "        # Create the directory if it does not exist\n",
    "        os.makedirs(subset_dir, exist_ok=True)\n",
    "        # Copy images from the original directory to the new subset directory\n",
    "        file_names = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
    "        for file_name in file_names:\n",
    "            shutil.copyfile(src=original_data_dir / file_name,\n",
    "                            dst=subset_dir / file_name)\n",
    "\n",
    "# Create subsets for training, validation, and testing\n",
    "create_data_subset(\"train\", start_index=0, end_index=1000)\n",
    "create_data_subset(\"validation\", start_index=1000, end_index=1500)\n",
    "create_data_subset(\"test\", start_index=1500, end_index=2500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_folder = pathlib.Path(subset_data_dir)\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    data_folder / \"train\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    data_folder / \"validation\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    data_folder / \"test\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2 EDA: Explore the data with relevant graphs, statistics and insights**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `1. Visualizing Sample Images`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "class_names = train_dataset.class_names\n",
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `2. Checking Class Distribution`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting number of dogs and cats in the training dataset\n",
    "label_count = {'dog': 0, 'cat': 0}\n",
    "for images, labels in train_dataset.unbatch()\n",
    "    label = class_names[labels.numpy()]\n",
    "    label_count[label] += 1\n",
    "\n",
    "plt.bar(label_count.keys(), label_count.values())\n",
    "plt.title('Class Distribution in Training Set')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `3. Image Sizes and Colors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_color_distribution(images, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    colors = ['Red', 'Green', 'Blue']\n",
    "    for i, color in enumerate(colors):\n",
    "        avg_color_intensity = np.mean(images[:,:,:,i], axis=(1, 2))\n",
    "        sns.histplot(avg_color_intensity, kde=True, color=color.lower(), label=f'{color} channel')\n",
    "    plt.title(f'Color distribution in {title}')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Intensity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "for images, labels in train_dataset.take(1): \n",
    "    plot_color_distribution(images, \"Training Images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `4. Image Pixel Intensity Distribution`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pixel_distribution(images, title):\n",
    "    pixel_values = images.numpy().flatten()\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(pixel_values, bins=50, kde=True)\n",
    "    plt.title(f'Pixel Intensity Distribution in {title}')\n",
    "    plt.xlabel('Pixel Intensity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "for images, _ in train_dataset.take(1):  \n",
    "    plot_pixel_distribution(images, \"Training Images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ` 5. Explore Image Augmentation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip('horizontal'),\n",
    "  tf.keras.layers.RandomRotation(0.1),\n",
    "  tf.keras.layers.RandomZoom(0.1),\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `6. data batch shape & labels batch shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_batch, labels_batch in train_dataset:\n",
    "    print(\"data batch shape:\", data_batch.shape)\n",
    "    print(\"labels batch shape:\", labels_batch.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3 Train two networks (make sure to use callbacks to save the best model version as done in lab 9):**\n",
    "#### 3.1 Define a Neural Network of your choice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model\n",
    "model_vanilla = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(180, 180, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vanilla.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vanilla.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vanilla.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"C:\\\\Foundations_of_Machine_Learning_Frameworks_lab\\\\Labs\\\\lab1\\\\CSCN8010-labs-simardeep-singh\\\\convnet_from_scratch.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_vanilla.fit(\n",
    "    train_dataset,\n",
    "    epochs=30,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Fine-Tune VGG16 (pre-trained on imagenet). Make sure to use validation to test for over-fitting. Plot the appropriate graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(180, 180, 3))\n",
    "base_model.trainable = False  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5), \n",
    "    layers.Dense(1, activation='sigmoid') \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"C:\\\\Foundations_of_Machine_Learning_Frameworks_lab\\\\Labs\\\\lab1\\\\CSCN8010-labs-simardeep-singh\\\\vgg16_fine_tuned.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=30,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, acc, label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, loss, label='Training Loss')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q4 Explore the relative performance of the models (make sure to load the best version of each model)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Loading best models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_model_path = 'C:\\\\Foundations_of_Machine_Learning_Frameworks_lab\\\\Labs\\\\lab1\\\\CSCN8010-labs-simardeep-singh\\\\convnet_from_scratch.keras'\n",
    "vanilla_model = tf.keras.models.load_model(vanilla_model_path)\n",
    "\n",
    "# UNABLE TO LOAD VGG16 MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = vanilla_model.evaluate(test_dataset)\n",
    "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **The vanilla model achieved a test accuracy of approximately 59.5%, with a loss of 0.6590. The accuracy metric indicates that the model correctly predicts the outcome (whether a sample belongs to one class or the other in a binary classification task) about 60.11% of the time during training and approximately 59.5% of the time on the unseen test data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Confusion Metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Get true labels and model predictions\n",
    "y_true = np.concatenate([labels.numpy() for _, labels in test_dataset])\n",
    "y_pred = np.round(vanilla_model.predict(test_dataset)).flatten()\n",
    "\n",
    "# Compute metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Class 0', 'Class 1']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **The model was evaluated over 63 batches, taking approximately 7 seconds with an average step time of 112 milliseconds. The confusion matrix reveals that out of 2000 total samples (1000 in each class), the model correctly predicted 449 instances of Class 0 and 536 instances of Class 1, leading to an overall accuracy of around 49.25% which means the model performs only slightly better than random guessing in this binary classification task.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `precision, recall, F1-score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "y_true = np.concatenate([y for _, y in test_dataset], axis=0)  \n",
    "y_pred_probs = vanilla_model.predict(test_dataset) \n",
    "\n",
    "threshold = 0.5\n",
    "y_pred = (y_pred_probs > threshold).astype(int)  \n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f'Precision: {precision:.3f}')\n",
    "print(f'Recall: {recall:.3f}')\n",
    "print(f'F1-Score: {f1:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-The model was evaluated over 63 batches, taking an average of 120 milliseconds per step, totaling approximately 8 seconds for the complete evaluation.**\n",
    "\n",
    "**-Precision: 0.481 - This means that when the model predicts the positive class, it is correct about 48.1% of the time. This is an indicator of the quality of the positive class predictions.**\n",
    "\n",
    "**-Recall: 0.523 - Signifying that the model correctly identifies 52.3% of all actual positive instances. This measures the model's ability to detect positive instances.**\n",
    "\n",
    "**-F1-Score: 0.501 - An F1-score of 0.501 suggests the model has moderate accuracy, with a balanced trade-off between precision and recall.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   `precision-recall curve.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "y_scores = vanilla_model.predict(test_dataset)  \n",
    "y_true = np.concatenate([y for x, y in test_dataset], axis=0)  \n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At the beginning of the threshold spectrum, we observe that the precision starts at 0.5 with a recall of 1. This indicates that when classifying almost all samples as positive, half are correct (since precision is the fraction of true positives among all predicted positives), and all true positives are captured (since recall is the fraction of true positives among all actual positives). As thresholds increase, both precision and recall show variations, which is expected as the classification criteria become stricter.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `CONCLUSION`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Accuracy:  the model seems to demonstrate moderate effectiveness. An accuracy around 50-60% was indicated, which suggests that the model performs slightly better than random guessing but is not highly reliable for making accurate predictions.\n",
    "\n",
    "2. Confusion Matrix: The confusion matrix shows a relatively even distribution of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN). However, the model seems to struggle equally with both classes, as indicated by the high number of false positives and false negatives. This suggests that the model does not strongly favor one class over the other but also fails to effectively distinguish between them in many cases.\n",
    "\n",
    "3. Precision and Recall: The model's precision and recall were low (around 0.48 and 0.52, respectively). This indicates that when the model predicts an instance as positive, it is correct less than half the time. Similarly, the model identifies just over half of all actual positive instances. These values suggest a lack of confidence in the modelâ€™s predictive power.\n",
    "\n",
    "4. F1-Score: The F1-score, being a harmonic mean of precision and recall, was also moderate (around 0.50). This score confirms that the model has a balanced but mediocre performance regarding precision and recall. The balance is not due to high performance but rather to a similar level of low performance on both metrics.\n",
    "\n",
    "5. Precision-Recall Curve: The precision-recall curve provide insight into the trade-off between precision and recall at different thresholds. However the curve suggested a moderate starting performance which deteriorates with increasing thresholds.\n",
    "\n",
    "**The model exhibits average performance, struggling to effectively differentiate between the classes with significant precision or recall. Its predictive power is marginally better than random chance but falls short of a desirable or reliable predictive model. The similar low rates of precision and recall indicate that there is no significant bias towards one class but rather a general difficulty in classification.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCN8010_classic_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
