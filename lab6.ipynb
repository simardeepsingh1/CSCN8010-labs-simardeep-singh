{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Lab 4 \n",
    "## Student Name: `Simardeep Singh`\n",
    "## Student Roll Number:`8976948`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q1 Obtain the data using the sklearn load_iris method. Redefine 2 classes for the target variable: virginica and non-virginica (the original dataset includes 3 classes).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Libraries \n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iris_dataset = load_iris()\n",
    "\n",
    "df = pd.DataFrame(data=iris_dataset.data,columns=iris_dataset.feature_names)\n",
    "df['target']=iris_dataset.target\n",
    "df['target']=df['target'].map({0:'non-virginica',1:'non-virginica',2:'virginica'})\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q2 Explore the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1: 1 table with descriptive statistics for each of the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive_statistics_ss = df.groupby('target').describe()\n",
    "print(descriptive_statistics_ss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2: 1 Histogram per feature, for each of the two classes. See https://seaborn.pydata.org/generated/seaborn.histplot.html and consider using the `hue` argument to distinguish between classes in the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "hist = sns.FacetGrid(df, col=\"target\", hue=\"target\", col_wrap=3, height=4)\n",
    "hist.map(sns.histplot, \"sepal length (cm)\", kde=True, bins=15, element='step')\n",
    "\n",
    "hist.set_titles(\"Histogram of {col_name}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3: Correlation matrix between the four features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c_relm = df[iris_dataset.feature_names].corr()\n",
    "sns.heatmap(c_relm, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix between Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4: At least 3 additional graphs. Use Kaggle for inspiration: go over the notebooks of that dataset, and choose graphs that you find most relevant for the problem statement. Add a reference (as a link) to the source of each graph. Explain (to the PM) what is the meaning of this graph, what can we learn from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   `1 Box Plot`\n",
    "[Link of kaggle example](https://www.kaggle.com/code/parulpandey/penguin-dataset-the-new-iris#Boxplot)\n",
    "\n",
    "##### Boxplot: Unveiling Feature Distribution Differences Between Classes\n",
    "A boxplot is an effective visualization for understanding the distribution of a specific feature within each class. It provides a clear representation of the median, quartiles, and potential outliers, offering insights into the central tendency and spread of the feature. In the context of our dataset, the boxplot for petal length assists in identifying differences in the distribution characteristics between 'virginica' and 'non-virginica' classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='target', y='petal length (cm)', data=df)\n",
    "plt.title('Boxplot of Petal Length for each Class')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `2 Pair Plot`\n",
    "[Link of kaggle example](https://www.kaggle.com/code/parulpandey/penguin-dataset-the-new-iris#Scatterplot)\n",
    "##### Pairplot: Understanding Feature Relationships and Class Distributions  \n",
    "A pairplot is a comprehensive visualization tool that displays scatter plots for each pair of features, histograms along the diagonal, and uses different colors to represent each class. This graph allows us to gain insights into how features are distributed concerning each other, helping identify patterns and separability between classes. It serves as a valuable tool for visualizing the overall structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue='target')\n",
    "plt.title('Pairplot of Features with Class Distributions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `3 Violin PLot`\n",
    "[Link of Kaggle example](https://www.kaggle.com/code/parulpandey/penguin-dataset-the-new-iris#Flipperlength-distribution)\n",
    "##### Violin Plot: Exploring Feature Distribution Shape and Density\n",
    "The violin plot, a combination of box plots and kernel density estimation, offers insights into both the shape and density of feature distributions. This graph is valuable for understanding the spread and central tendency of a feature across different classes. Specifically, the violin plot for sepal length aids in identifying differences in distribution characteristics between 'virginica' and 'non-virginica' classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='target', y='sepal length (cm)', data=df)\n",
    "plt.title('Sepal Length for each Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5: At least 3 insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Insight 1  Descriptive statistics :`\n",
    "\n",
    "#### Mean Sepal Length:\n",
    "- Non-Virginica: 5.471\n",
    "- Virginica: 6.588\n",
    "- **Observation:** Virginica generally has a higher mean sepal length compared to Non-Virginica.\n",
    "\n",
    "#### Variability in Sepal Length:\n",
    "- **Standard Deviation:**\n",
    "  - Non-Virginica: 0.641698\n",
    "  - Virginica: 0.635880\n",
    "- **Observation:** Both classes exhibit relatively similar variability in sepal length.\n",
    "\n",
    "#### Range of Sepal Length:\n",
    "- **Min to Max:**\n",
    "  - Non-Virginica: 4.3 to 7.0\n",
    "  - Virginica: 4.9 to 7.9\n",
    "- **Observation:** Virginica has a broader range of sepal length compared to Non-Virginica.\n",
    "\n",
    "#### Interquartile Range (IQR) of Sepal Length:\n",
    "- **IQR:**\n",
    "  - Non-Virginica: 0.9 (5.9 - 5.0)\n",
    "  - Virginica: 0.675 (6.9 - 6.225)\n",
    "- **Observation:** The IQR for Non-Virginica is wider, indicating a more spread distribution compared to Virginica.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Insight 2  Correlation Matrix:`\n",
    "\n",
    "### 1. Negative Correlation Between Sepal Length and Sepal Width:\n",
    "- **Correlation Coefficient:** -0.117570\n",
    "- **Interpretation:** A weak negative correlation suggests that as sepal length increases, sepal width tends to decrease slightly.\n",
    "\n",
    "### 2. Strong Positive Correlation Between Sepal Length and Petal Length:\n",
    "- **Correlation Coefficient:** 0.871754\n",
    "- **Interpretation:** A strong positive correlation indicates that as sepal length increases, petal length tends to increase significantly.\n",
    "\n",
    "### 3. Strong Positive Correlation Between Sepal Length and Petal Width:\n",
    "- **Correlation Coefficient:** 0.817941\n",
    "- **Interpretation:** A strong positive correlation suggests that as sepal length increases, petal width tends to increase significantly.\n",
    "\n",
    "### 4. Moderate Negative Correlation Between Sepal Width and Petal Length:\n",
    "- **Correlation Coefficient:** -0.428440\n",
    "- **Interpretation:** A moderate negative correlation implies that as sepal width increases, petal length tends to decrease moderately.\n",
    "\n",
    "### 5. Weak Negative Correlation Between Sepal Width and Petal Width:\n",
    "- **Correlation Coefficient:** -0.366126\n",
    "- **Interpretation:** A weak negative correlation suggests that as sepal width increases, petal width tends to decrease slightly.\n",
    "\n",
    "### 6. Very Strong Positive Correlation Between Petal Length and Petal Width:\n",
    "- **Correlation Coefficient:** 0.962865\n",
    "- **Interpretation:** A very strong positive correlation indicates that as petal length increases, petal width tends to increase significantly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Insight 3  Outliers:`\n",
    "\n",
    "### 1. High Outliers in Sepal Length:\n",
    "   - **Percentage:** ~66.67%\n",
    "   - **Insight:** Indicates notable deviations in sepal length, prompting further investigation into the nature of these outliers.\n",
    "\n",
    "### 2. Unusual Outlier Percentage in Sepal Width:\n",
    "   - **Percentage:** ~266.67%\n",
    "   - **Insight:** The high percentage in sepal width outliers requires a review of the outlier detection methodology for accuracy.\n",
    "\n",
    "### 3. No Outliers in Petal Length and Petal Width:\n",
    "   - **Percentage:** 0.00%\n",
    "   - **Insight:** Both petal length and petal width show no identified outliers, suggesting adherence to the main distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q3 Split the data to a train set (120 records), a validation set (15 records)  and a test set (15 records).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[iris_dataset.feature_names]\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "X_val_named = pd.DataFrame(X_val, columns=iris_dataset.feature_names)\n",
    "\n",
    "\n",
    "print(\"Shape of Train set:\", X_train.shape, y_train.shape)\n",
    "print(\"Shape of Validation set :\", X_val.shape, y_val.shape)\n",
    "print(\"Shape of Test set :\", X_test.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q4 Run four logistic regression models models, with 1,2,3 and 4 features. Choose the order in random, or based on some reasoning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = iris_dataset.feature_names\n",
    "\n",
    "model_results = []\n",
    "\n",
    "for i in range(1, 5):\n",
    "    selected_features = all_features[:i]\n",
    "    \n",
    "    model = LogisticRegression(fit_intercept=False).fit(X_train_named[selected_features], y_train)\n",
    "\n",
    "    model_results.append({'model': model, 'features': selected_features})\n",
    "\n",
    "    accuracy_val = accuracy_score(y_val, model.predict(X_val_named[selected_features]))    \n",
    "    print(f\"Model {i} with {i} feature - Validation Accuracy: {accuracy_val:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q5 Evaluate the models on the validation set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1 Use the predict and predict_proba methods to list in a table how well each model is doing for each of the instances in the validation set. There should be one table per model. Each table should have four columns: instance number, probability of predicting verginica, actual prediction by the model, ground truth. (0.5 point) Next, summarize the data in each table to a single measure (number) per model. What would you use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "for result_info in model_results:\n",
    "    model = result_info['model']\n",
    "    features = result_info['features']\n",
    "\n",
    "    y_prob_val = model.predict_proba(X_val_named[features])[:, 1]\n",
    "    y_pred_val = model.predict(X_val_named[features])\n",
    "\n",
    "    result_df = pd.DataFrame({\n",
    "        'Instance': range(1, len(X_val) + 1),\n",
    "        'Probability of Predicting Virginica': y_prob_val,\n",
    "        'Predicted Class': y_pred_val,\n",
    "        'Ground Truth': y_val\n",
    "    })\n",
    "\n",
    "    print(f\"Table for Model with {len(features)} Feature(s):\")\n",
    "    print(result_df)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    roc_auc_val = roc_auc_score(y_val, y_prob_val)\n",
    "    accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "    try:\n",
    "        precision_val = precision_score(y_val, y_pred_val, pos_label='virginica')\n",
    "    except UndefinedMetricWarning:\n",
    "        precision_val = \"Undefined (no predicted positives)\"\n",
    "\n",
    "    recall_val = recall_score(y_val, y_pred_val, pos_label='virginica')\n",
    "    f1_val = f1_score(y_val, y_pred_val, pos_label='virginica')\n",
    "    \n",
    "    print(f\"Summary Metrics for Model with {len(features)} Feature(s):\")\n",
    "    print(f\"ROC-AUC: {roc_auc_val:.2%}\")\n",
    "    print(f\"Accuracy: {accuracy_val:.2%}\")\n",
    "    print(f\"Precision: {precision_val}\")\n",
    "    print(f\"Recall: {recall_val:.2%}\")\n",
    "    print(f\"F1 Score: {f1_val:.2%}\")\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2 Plot the decision boundary for three models (with 1, 2 and 3 features; 3 plots), together with the validation data. Tip: using Plotly will allow you to rotate the 3D graph with the 3 features. Tip: for the code required to find the decision boundary you can look at this notebook in the course repository "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "\n",
    "for i, result_info in enumerate(model_results):\n",
    "    model = result_info['model']\n",
    "    features = result_info['features']\n",
    "\n",
    "    \n",
    "    if len(features) == 1:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=X_val[features[0]],\n",
    "            y=y_val,\n",
    "            mode='markers',\n",
    "            marker=dict(color='blue', opacity=0.8),\n",
    "            name=f'Model {i + 1} with {len(features)} Feature(s)'\n",
    "        ))\n",
    "    elif len(features) == 2:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=X_val[features[0]],\n",
    "            y=X_val[features[1]],\n",
    "            mode='markers',\n",
    "            marker=dict(color='blue', opacity=0.8),\n",
    "            name=f'Model {i + 1} with {len(features)} Feature(s)'\n",
    "        ))\n",
    "    elif len(features) >= 3:\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=X_val[features[0]],\n",
    "            y=X_val[features[1]],\n",
    "            z=X_val[features[2]],\n",
    "            mode='markers',\n",
    "            marker=dict(color='blue', opacity=0.8),\n",
    "            name=f'Model {i + 1} with {len(features)} Feature(s)'\n",
    "        ))\n",
    "\n",
    "        \n",
    "        if len(features) == 3:\n",
    "            plot_decision_boundary_3d(model, features, fig)\n",
    "\n",
    "\n",
    "fig.update_layout(scene=dict(aspectmode=\"cube\"))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3 Failure modes: using the two preceding steps, in which types of data instances is each model wrong? Can you identify some failure patterns within a model or across models?\n",
    "\n",
    "True Positives: 8 instances\n",
    "The model correctly predicted 8 instances as positive.\n",
    "\n",
    "True Negatives: 7 instances\n",
    "The model correctly predicted 7 instances as negative.\n",
    "\n",
    "False Positives: 0 instances\n",
    "The model incorrectly predicted 0 instances as positive when they were actually negative.\n",
    "\n",
    "False Negatives: 0 instances\n",
    "The model incorrectly predicted 0 instances as negative when they were actually positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_val, y_pred_val)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6 Recommend the best model (provide reasoning). Summarize the results of this model on the test set. Tip: you can use the \"single-number\" measure you used on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 4 with four features is the best-performing model based on the validation results, achieving perfect accuracy (100.00%), ROC-AUC (100.00%), precision (1.0), recall (100.00%), and F1 Score (100.00%).\n",
    "\n",
    "##### `Test result`\n",
    "The best model for predicting the 'virginica' class on the test set was Model 4, which used all four features. It performed really well, with 100% accuracy, ROC-AUC, precision, recall, and F1 Score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model_results[3]['model']\n",
    "\n",
    "\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "y_prob_test = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "roc_auc_test = roc_auc_score(y_test, y_prob_test)\n",
    "precision_test = precision_score(y_test, y_pred_test, pos_label='virginica')\n",
    "recall_test = recall_score(y_test, y_pred_test, pos_label='virginica')\n",
    "f1_test = f1_score(y_test, y_pred_test, pos_label='virginica')\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy_test:.2%}\")\n",
    "print(f\"Test ROC-AUC: {roc_auc_test:.2%}\")\n",
    "print(f\"Test Precision: {precision_test:.2%}\")\n",
    "print(f\"Test Recall: {recall_test:.2%}\")\n",
    "print(f\"Test F1 Score: {f1_test:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCN8010_classic_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
